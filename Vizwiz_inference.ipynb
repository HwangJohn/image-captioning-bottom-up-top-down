{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "from datasets import *\n",
    "from utils import *\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from nlgeval import NLGEval\n",
    "import os\n",
    "from glob import glob\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"3\"\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "data_folder = 'final_dataset'  # folder with data files saved by create_input_files.py\n",
    "data_name = 'coco_5_cap_per_img_5_min_word_freq'  # base name shared by data files\n",
    "checkpoint_file = 'BEST_48checkpoint_coco_5_cap_per_img_5_min_word_freq.pth.tar'  # model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\n",
    "cudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderWithAttention(\n",
       "  (attention): Attention(\n",
       "    (features_att): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    (decoder_att): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (full_att): Linear(in_features=1024, out_features=1, bias=True)\n",
       "    (relu): ReLU()\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (softmax): Softmax(dim=1)\n",
       "  )\n",
       "  (embedding): Embedding(9490, 1024)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (top_down_attention): LSTMCell(4096, 1024)\n",
       "  (language_model): LSTMCell(3072, 1024)\n",
       "  (fc1): Linear(in_features=1024, out_features=9490, bias=True)\n",
       "  (fc): Linear(in_features=1024, out_features=9490, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "torch.nn.Module.dump_patches = True\n",
    "checkpoint = torch.load(checkpoint_file,map_location = device)\n",
    "decoder = checkpoint['decoder']\n",
    "decoder = decoder.to(device)\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlgeval = NLGEval()  # loads the evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load word map (word2ix)\n",
    "word_map_file = os.path.join(data_folder, 'WORDMAP_' + data_name + '.json')\n",
    "with open(word_map_file, 'r') as j:\n",
    "    word_map = json.load(j)\n",
    "rev_word_map = {v: k for k, v in word_map.items()}\n",
    "vocab_size = len(word_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vizwiz test dataset\n",
    "# test_paths = glob(\"../mypythia/data/vizwiz/test/*.jpg\")\n",
    "test_paths = glob(\"../mypythia/data/vizwiz/test_npy/*.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(22.8031))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_features = np.load(test_paths[7])[:36]\n",
    "image_features = np.expand_dims(image_features, axis=0)\n",
    "image_features = torch.tensor(image_features, dtype=torch.float32)\n",
    "torch.min(image_features), torch.max(image_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_paths = sorted(test_paths, key=lambda x: int(os.path.split(x)[-1].split(\"_\")[-1].split(\".\")[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_size = 100\n",
    "k = beam_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000000.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000001.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000002.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000003.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000004.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000005.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000006.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000007.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000008.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000009.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000010.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000011.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000012.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000013.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000014.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000015.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000016.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000017.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000018.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000019.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000020.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000021.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000022.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000023.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000024.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000025.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000026.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000027.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000028.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000029.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000030.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000031.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000032.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000033.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000034.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000035.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000036.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000037.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000038.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000039.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000040.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000041.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000042.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000043.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000044.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000045.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000046.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000047.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000048.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000049.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000050.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000051.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000052.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000053.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000054.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000055.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000056.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000057.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000058.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000059.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000060.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000061.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000062.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000063.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000064.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000065.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000066.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000067.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000068.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000069.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000070.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000071.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000072.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000073.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000074.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000075.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000076.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000077.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000078.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000079.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000080.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000081.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000082.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000083.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000084.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000085.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000086.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000087.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000088.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000089.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000090.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000091.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000092.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000093.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000094.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000095.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000096.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000097.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000098.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000099.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000100.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000101.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000102.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000103.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000104.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000105.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000106.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000107.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000108.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000109.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000110.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000111.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000112.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000113.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000114.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000115.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000116.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000117.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000118.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000119.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000120.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000121.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000122.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000123.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000124.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000125.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000126.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000127.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000128.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000129.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000130.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000131.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000132.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000133.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000134.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000135.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000136.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000137.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000138.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000139.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000140.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000141.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000142.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000143.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000144.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000145.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000146.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000147.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000148.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000149.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000150.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000151.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000152.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000153.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000154.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000155.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000156.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000157.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000158.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000159.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000160.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000161.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000162.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000163.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000164.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000165.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000166.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000167.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000168.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000169.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000170.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000171.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000172.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000173.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000174.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000175.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000176.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000177.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000178.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000179.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000180.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000181.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000182.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000183.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000184.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000185.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000186.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000187.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000188.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000189.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000190.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000191.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000192.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000193.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000194.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000195.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000196.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000197.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000198.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000199.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000200.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000201.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000202.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000203.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000204.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000205.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000206.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000207.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000208.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000209.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000210.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000211.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000212.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000213.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000214.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000215.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000216.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000217.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000218.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000219.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000220.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000221.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000222.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000223.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000224.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000225.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000226.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000227.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000228.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000229.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000230.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000231.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000232.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000233.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000234.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000235.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000236.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000237.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000238.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000239.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000240.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000241.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000242.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000243.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000244.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000245.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000246.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000247.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000248.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000249.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000250.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000251.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000252.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000253.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000254.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000255.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000256.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000257.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000258.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000259.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000260.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000261.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000262.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000263.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000264.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000265.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000266.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000267.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000268.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000269.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000270.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000271.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000272.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000273.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000274.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000275.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000276.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000277.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000278.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000279.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000280.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000281.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000282.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000283.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000284.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000285.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000286.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000287.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000288.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000289.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000290.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000291.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000292.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000293.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000294.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000295.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000296.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000297.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000298.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000299.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000300.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000301.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000302.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000303.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000304.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000305.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000306.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000307.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000308.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000309.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000310.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000311.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000312.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000313.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000314.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000315.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000316.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000317.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000318.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000319.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000320.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000321.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000322.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000323.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000324.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000325.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000326.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000327.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000328.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000329.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000330.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000331.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000332.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000333.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000334.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000335.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000336.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000337.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000338.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000339.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000340.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000341.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000342.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000343.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000344.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000345.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000346.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000347.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000348.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000349.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000350.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000351.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000352.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000353.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000354.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000355.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000356.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000357.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000358.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000359.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000360.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000361.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000362.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000363.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000364.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000365.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000366.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000367.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000368.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000369.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000370.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000371.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000372.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000373.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000374.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000375.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000376.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000377.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000378.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000379.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000380.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000381.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000382.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000383.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000384.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000385.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000386.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000387.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000388.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000389.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000390.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000391.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000392.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000393.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000394.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000395.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000396.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000397.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000398.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000399.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000400.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000401.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000402.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000403.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000404.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000405.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000406.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000407.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000408.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000409.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000410.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000411.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000412.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000413.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000414.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000415.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000416.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000417.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000418.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000419.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000420.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000421.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000422.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000423.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000424.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000425.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000426.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000427.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000428.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000429.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000430.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000431.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000432.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000433.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000434.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000435.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000436.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000437.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000438.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000439.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000440.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000441.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000442.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000443.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000444.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000445.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000446.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000447.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000448.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000449.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000450.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000451.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000452.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000453.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000454.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000455.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000456.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000457.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000458.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000459.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000460.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000461.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000462.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000463.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000464.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000465.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000466.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000467.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000468.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000469.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000470.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000471.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000472.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000473.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000474.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000475.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000476.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000477.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000478.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000479.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000480.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000481.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000482.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000483.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000484.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000485.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000486.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000487.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000488.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000489.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000490.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000491.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000492.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000493.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000494.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000495.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000496.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000497.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000498.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000499.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000500.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000501.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000502.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000503.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000504.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000505.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000506.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000507.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000508.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000509.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000510.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000511.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000512.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000513.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000514.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000515.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000516.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000517.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000518.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000519.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000520.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000521.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000522.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000523.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000524.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000525.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000526.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000527.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000528.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000529.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000530.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000531.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000532.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000533.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000534.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000535.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000536.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000537.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000538.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000539.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000540.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000541.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000542.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000543.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000544.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000545.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000546.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000547.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000548.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000549.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000550.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000551.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000552.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000553.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000554.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000555.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000556.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000557.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000558.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000559.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000560.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000561.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000562.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000563.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000564.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000565.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000566.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000567.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000568.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000569.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000570.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000571.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000572.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000573.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000574.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000575.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000576.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000577.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000578.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000579.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000580.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000581.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000582.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000583.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000584.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000585.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000586.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000587.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000588.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000589.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000590.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000591.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000592.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000593.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000594.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000595.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000596.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000597.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000598.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000599.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000600.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000601.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000602.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000603.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000604.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000605.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000606.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000607.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000608.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000609.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000610.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000611.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000612.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000613.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000614.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000615.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000616.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000617.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000618.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000619.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000620.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000621.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000622.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000623.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000624.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000625.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000626.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000627.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000628.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000629.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000630.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000631.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000632.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000633.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000634.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000635.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000636.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000637.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000638.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000639.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000640.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000641.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000642.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000643.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000644.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000645.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000646.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000647.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000648.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000649.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000650.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000651.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000652.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000653.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000654.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000655.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000656.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000657.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000658.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000659.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000660.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000661.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000662.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000663.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000664.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000665.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000666.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000667.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000668.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000669.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000670.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000671.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000672.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000673.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000674.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000675.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000676.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000677.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000678.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000679.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000680.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000681.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000682.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000683.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000684.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000685.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000686.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000687.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000688.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000689.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000690.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000691.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000692.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000693.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000694.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000695.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000696.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000697.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000698.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000699.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000700.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000701.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000702.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000703.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000704.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000705.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000706.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000707.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000708.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000709.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000710.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000711.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000712.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000713.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000714.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000715.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000716.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000717.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000718.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000719.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000720.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000721.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000722.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000723.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000724.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000725.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000726.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000727.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000728.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000729.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000730.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000731.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000732.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000733.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000734.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000735.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000736.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000737.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000738.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000739.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000740.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000741.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000742.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000743.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000744.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000745.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000746.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000747.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000748.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000749.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000750.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000751.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000752.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000753.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000754.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000755.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000756.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000757.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000758.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000759.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000760.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000761.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000762.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000763.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000764.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000765.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000766.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000767.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000768.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000769.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000770.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000771.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000772.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000773.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000774.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000775.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000776.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000777.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000778.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000779.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000780.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000781.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000782.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000783.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000784.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000785.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000786.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000787.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000788.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000789.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000790.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000791.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000792.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000793.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000794.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000795.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000796.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000797.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000798.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000799.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000800.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000801.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000802.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000803.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000804.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000805.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000806.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000807.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000808.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000809.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000810.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000811.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000812.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000813.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000814.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000815.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000816.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000817.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000818.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000819.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000820.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000821.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000822.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000823.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000824.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000825.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000826.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000827.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000828.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000829.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000830.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000831.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000832.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000833.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000834.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000835.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000836.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000837.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000838.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000839.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000840.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000841.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000842.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000843.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000844.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000845.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000846.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000847.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000848.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000849.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000850.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000851.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000852.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000853.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000854.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000855.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000856.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000857.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000858.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000859.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000860.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000861.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000862.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000863.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000864.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000865.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000866.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000867.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000868.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000869.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000870.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000871.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000872.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000873.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000874.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000875.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000876.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000877.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000878.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000879.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000880.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000881.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000882.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000883.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000884.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000885.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000886.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000887.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000888.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000889.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000890.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000891.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000892.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000893.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000894.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000895.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000896.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000897.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000898.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000899.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000900.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000901.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000902.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000903.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000904.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000905.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000906.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000907.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000908.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000909.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000910.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000911.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000912.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000913.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000914.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000915.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000916.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000917.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000918.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000919.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000920.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000921.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000922.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000923.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000924.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000925.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000926.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000927.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000928.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000929.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000930.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000931.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000932.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000933.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000934.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000935.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000936.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000937.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000938.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000939.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000940.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000941.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000942.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000943.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000944.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000945.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000946.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000947.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000948.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000949.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000950.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000951.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000952.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000953.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000954.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000955.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000956.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000957.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000958.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000959.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000960.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000961.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000962.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000963.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000964.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000965.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000966.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000967.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000968.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000969.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000970.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000971.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000972.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000973.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000974.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000975.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000976.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000977.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000978.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000979.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000980.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000981.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000982.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000983.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000984.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000985.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000986.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000987.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000988.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000989.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000990.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000991.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000992.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000993.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000994.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000995.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000996.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000997.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000998.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00000999.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001000.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001001.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001002.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001003.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001004.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001005.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001006.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001007.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001008.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001009.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001010.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001011.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001012.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001013.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001014.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001015.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001016.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001017.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001018.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001019.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001020.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001021.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001022.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001023.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001024.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001025.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001026.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001027.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001028.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001029.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001030.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001031.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001032.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001033.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001034.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001035.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001036.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001037.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001038.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001039.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001040.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001041.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001042.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001043.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001044.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001045.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001046.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001047.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001048.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001049.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001050.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001051.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001052.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001053.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001054.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001055.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001056.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001057.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001058.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001059.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001060.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001061.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001062.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001063.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001064.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001065.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001066.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001067.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001068.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001069.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001070.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001071.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001072.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001073.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001074.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001075.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001076.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001077.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001078.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001079.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001080.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001081.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001082.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001083.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001084.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001085.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001086.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001087.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001088.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001089.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001090.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001091.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001092.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001093.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001094.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001095.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001096.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001097.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001098.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001099.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001100.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001101.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001102.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001103.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001104.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001105.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001106.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001107.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001108.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001109.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001110.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001111.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001112.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001113.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001114.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001115.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001116.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001117.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001118.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001119.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001120.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001121.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001122.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001123.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001124.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001125.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001126.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001127.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001128.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001129.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001130.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001131.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001132.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001133.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001134.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001135.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001136.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001137.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001138.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001139.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001140.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001141.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001142.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001143.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001144.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001145.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001146.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001147.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001148.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001149.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001150.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001151.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001152.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001153.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001154.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001155.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001156.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001157.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001158.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001159.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001160.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001161.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001162.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001163.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001164.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001165.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001166.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001167.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001168.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001169.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001170.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001171.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001172.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001173.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001174.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001175.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001176.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001177.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001178.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001179.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001180.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n",
      "image_file_name: '../mypythia/data/vizwiz/test_npy/VizWiz_test_00001181.npy'\n",
      "image_features_mean: torch.Size([1, 2048])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-5b8262ddca41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m# Which sequences are incomplete (didn't reach <end>)?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n\u001b[0m\u001b[1;32m     69\u001b[0m                            next_word != word_map['<end>']]\n\u001b[1;32m     70\u001b[0m         \u001b[0mcomplete_inds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_word_inds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mincomplete_inds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-5b8262ddca41>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m# Which sequences are incomplete (didn't reach <end>)?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n\u001b[0;32m---> 69\u001b[0;31m                            next_word != word_map['<end>']]\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mcomplete_inds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_word_inds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mincomplete_inds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for x in test_paths:\n",
    "    print(f\"image_file_name: {x!r}\")\n",
    "    image_features = np.load(x)[:36]\n",
    "    image_features = np.expand_dims(image_features, axis=0)\n",
    "    image_features = torch.tensor(image_features, dtype=torch.float32)\n",
    "\n",
    "    references = list()\n",
    "    hypotheses = list()\n",
    "\n",
    "    k = beam_size\n",
    "\n",
    "    # Move to GPU device, if available\n",
    "    image_features = image_features.to(device)  # (1, 36, 2048)\n",
    "    image_features_mean = image_features.mean(1)\n",
    "    print(f\"image_features_mean: {image_features_mean.shape!r}\")\n",
    "\n",
    "    image_features_mean = image_features_mean.expand(k,2048)\n",
    "\n",
    "    # Tensor to store top k previous words at each step; now they're just <start>\n",
    "    k_prev_words = torch.LongTensor([[word_map['<start>']]] * k).to(device)  # (k, 1)\n",
    "\n",
    "    # Tensor to store top k sequences; now they're just <start>\n",
    "    seqs = k_prev_words  # (k, 1)\n",
    "\n",
    "    # Tensor to store top k sequences' scores; now they're just 0\n",
    "    top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n",
    "\n",
    "    # Lists to store completed sequences and scores\n",
    "    complete_seqs = list()\n",
    "    complete_seqs_scores = list()\n",
    "\n",
    "    # Start decoding\n",
    "    step = 1\n",
    "    h1, c1 = decoder.init_hidden_state(k)  # (batch_size, decoder_dim)\n",
    "    h2, c2 = decoder.init_hidden_state(k)\n",
    "\n",
    "    # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n",
    "    while True:\n",
    "        embeddings = decoder.embedding(k_prev_words).squeeze(1)  # (s, embed_dim)\n",
    "        h1,c1 = decoder.top_down_attention(\n",
    "            torch.cat([h2,image_features_mean,embeddings], dim=1),\n",
    "            (h1,c1))  # (batch_size_t, decoder_dim)\n",
    "        attention_weighted_encoding = decoder.attention(image_features,h1)\n",
    "        h2,c2 = decoder.language_model(\n",
    "            torch.cat([attention_weighted_encoding,h1], dim=1),(h2,c2))\n",
    "\n",
    "        scores = decoder.fc(h2)  # (s, vocab_size)\n",
    "        scores = F.log_softmax(scores, dim=1)\n",
    "\n",
    "        # Add\n",
    "        scores = top_k_scores.expand_as(scores) + scores  # (s, vocab_size)\n",
    "\n",
    "        # For the first step, all k points will have the same scores (since same k previous words, h, c)\n",
    "        if step == 1:\n",
    "            top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (s)\n",
    "        else:\n",
    "            # Unroll and find top scores, and their unrolled indices\n",
    "            top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  # (s)\n",
    "\n",
    "        # Convert unrolled indices to actual indices of scores\n",
    "        prev_word_inds = top_k_words / vocab_size  # (s)\n",
    "        next_word_inds = top_k_words % vocab_size  # (s)\n",
    "\n",
    "        # Add new words to sequences\n",
    "        seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n",
    "\n",
    "        # Which sequences are incomplete (didn't reach <end>)?\n",
    "        incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n",
    "                           next_word != word_map['<end>']]\n",
    "        complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
    "\n",
    "        # Set aside complete sequences\n",
    "        if len(complete_inds) > 0:\n",
    "            complete_seqs.extend(seqs[complete_inds].tolist())\n",
    "            complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
    "        k -= len(complete_inds)  # reduce beam length accordingly\n",
    "\n",
    "        # Proceed with incomplete sequences\n",
    "        if k == 0:\n",
    "            break\n",
    "        seqs = seqs[incomplete_inds]\n",
    "        h1 = h1[prev_word_inds[incomplete_inds]]\n",
    "        c1 = c1[prev_word_inds[incomplete_inds]]\n",
    "        h2 = h2[prev_word_inds[incomplete_inds]]\n",
    "        c2 = c2[prev_word_inds[incomplete_inds]]\n",
    "        image_features_mean = image_features_mean[prev_word_inds[incomplete_inds]]\n",
    "        top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
    "        k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
    "        \n",
    "        \n",
    "\n",
    "        # Break if things have been going on too long\n",
    "        if step > 50:\n",
    "            break\n",
    "        step += 1\n",
    "        \n",
    "    if len(complete_seqs) > 0:\n",
    "        break\n",
    "        \n",
    "#     i = complete_seqs_scores.index(max(complete_seqs_scores))\n",
    "#     seq = complete_seqs[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 100)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(complete_inds), len(incomplete_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 52])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 9490])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_word_map[9487]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9487, 9487, 9487, 9487, 9487])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(scores.cpu().detach().numpy(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9488, 7961, 7961, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487,\n",
       "         9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487,\n",
       "         9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487,\n",
       "         9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487,\n",
       "         9487, 9487, 9487, 9487],\n",
       "        [9488, 7961, 7961, 9487, 9487, 9487, 9487, 9487, 7961, 9487, 9487, 9487,\n",
       "         9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487,\n",
       "         9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487,\n",
       "         9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487,\n",
       "         9487, 9487, 9487, 9487],\n",
       "        [9488, 7961, 7961, 9487, 9487, 9487, 9487, 7961, 9487, 9487, 9487, 9487,\n",
       "         9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487,\n",
       "         9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487,\n",
       "         9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487,\n",
       "         9487, 9487, 9487, 9487],\n",
       "        [9488, 7961, 7961, 9487, 9487, 9487, 9487, 9487, 9487, 7961, 9487, 9487,\n",
       "         9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487,\n",
       "         9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487,\n",
       "         9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487,\n",
       "         9487, 9487, 9487, 9487],\n",
       "        [9488, 7961, 7961, 9487, 9487, 9487, 7961, 9487, 9487, 9487, 9487, 9487,\n",
       "         9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487,\n",
       "         9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487,\n",
       "         9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487, 9487,\n",
       "         9487, 9487, 9487, 9487]], device='cuda:0')"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9487, 9487, 9487, 9487, 9487], device='cuda:0')"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_word_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incomplete_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "from datasets import *\n",
    "from utils import *\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from nlgeval import NLGEval\n",
    "\n",
    "# Parameters\n",
    "data_folder = 'final_dataset'  # folder with data files saved by create_input_files.py\n",
    "data_name = 'coco_5_cap_per_img_5_min_word_freq'  # base name shared by data files\n",
    "checkpoint_file = 'BEST_35checkpoint_coco_5_cap_per_img_5_min_word_freq.pth.tar'  # model checkpoint\n",
    "\n",
    "word_map_file = 'WORDMAP_coco_5_cap_per_img_5_min_word_freq.json'  # word map, ensure it's the same the data was encoded with and the model was trained with\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\n",
    "cudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n",
    "\n",
    "# Load model\n",
    "torch.nn.Module.dump_patches = True\n",
    "checkpoint = torch.load(checkpoint_file,map_location = device)\n",
    "decoder = checkpoint['decoder']\n",
    "decoder = decoder.to(device)\n",
    "decoder.eval()\n",
    "\n",
    "nlgeval = NLGEval()  # loads the evaluator\n",
    "\n",
    "# Load word map (word2ix)\n",
    "word_map_file = os.path.join(data_folder, 'WORDMAP_' + data_name + '.json')\n",
    "with open(word_map_file, 'r') as j:\n",
    "    word_map = json.load(j)\n",
    "rev_word_map = {v: k for k, v in word_map.items()}\n",
    "vocab_size = len(word_map)\n",
    "\n",
    "def evaluate(beam_size):\n",
    "    \"\"\"\n",
    "    Evaluation\n",
    "    :param beam_size: beam size at which to generate captions for evaluation\n",
    "    :return: Official MSCOCO evaluator scores - bleu4, cider, rouge, meteor\n",
    "    \"\"\"\n",
    "    # DataLoader\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        CaptionDataset(data_folder, data_name, 'TEST'),\n",
    "        batch_size=1, shuffle=True, num_workers=1, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "    # Lists to store references (true captions), and hypothesis (prediction) for each image\n",
    "    # If for n images, we have n hypotheses, and references a, b, c... for each image, we need -\n",
    "    # references = [[ref1a, ref1b, ref1c], [ref2a, ref2b], ...], hypotheses = [hyp1, hyp2, ...]\n",
    "    references = list()\n",
    "    hypotheses = list()\n",
    "\n",
    "    # For each image\n",
    "    for i, (image_features, caps, caplens, allcaps) in enumerate(\n",
    "            tqdm(loader, desc=\"EVALUATING AT BEAM SIZE \" + str(beam_size))):\n",
    "        if i > 2:\n",
    "            break\n",
    "\n",
    "        k = beam_size\n",
    "\n",
    "        # Move to GPU device, if available\n",
    "        print(f\"image_features.shape: {image_features.shape!r}\")\n",
    "        print(f\"np.min(image_features): {torch.min(image_features)!r}, np.max(image_features): {torch.max(image_features)!r}\")\n",
    "        image_features = image_features.to(device)  # (1, 36, 2048)\n",
    "        image_features_mean = image_features.mean(1)\n",
    "        print(f\"image_features_mean.shape: {image_features_mean.shape!r}\")\n",
    "        image_features_mean = image_features_mean.expand(k,2048)\n",
    "        print(f\"image_features_mean.shape: {image_features_mean.shape!r}\")        \n",
    "\n",
    "        # Tensor to store top k previous words at each step; now they're just <start>\n",
    "        k_prev_words = torch.LongTensor([[word_map['<start>']]] * k).to(device)  # (k, 1)\n",
    "\n",
    "        # Tensor to store top k sequences; now they're just <start>\n",
    "        seqs = k_prev_words  # (k, 1)\n",
    "\n",
    "        # Tensor to store top k sequences' scores; now they're just 0\n",
    "        top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n",
    "\n",
    "        # Lists to store completed sequences and scores\n",
    "        complete_seqs = list()\n",
    "        complete_seqs_scores = list()\n",
    "\n",
    "        # Start decoding\n",
    "        step = 1\n",
    "        h1, c1 = decoder.init_hidden_state(k)  # (batch_size, decoder_dim)\n",
    "        h2, c2 = decoder.init_hidden_state(k)\n",
    "\n",
    "        # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n",
    "        while True:\n",
    "\n",
    "            embeddings = decoder.embedding(k_prev_words).squeeze(1)  # (s, embed_dim)\n",
    "            h1,c1 = decoder.top_down_attention(\n",
    "                torch.cat([h2,image_features_mean,embeddings], dim=1),\n",
    "                (h1,c1))  # (batch_size_t, decoder_dim)\n",
    "            attention_weighted_encoding = decoder.attention(image_features,h1)\n",
    "            h2,c2 = decoder.language_model(\n",
    "                torch.cat([attention_weighted_encoding,h1], dim=1),(h2,c2))\n",
    "\n",
    "            scores = decoder.fc(h2)  # (s, vocab_size)\n",
    "            scores = F.log_softmax(scores, dim=1)\n",
    "\n",
    "            # Add\n",
    "            scores = top_k_scores.expand_as(scores) + scores  # (s, vocab_size)\n",
    "\n",
    "            # For the first step, all k points will have the same scores (since same k previous words, h, c)\n",
    "            if step == 1:\n",
    "                top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (s)\n",
    "            else:\n",
    "                # Unroll and find top scores, and their unrolled indices\n",
    "                top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  # (s)\n",
    "\n",
    "            # Convert unrolled indices to actual indices of scores\n",
    "            prev_word_inds = top_k_words / vocab_size  # (s)\n",
    "            next_word_inds = top_k_words % vocab_size  # (s)\n",
    "\n",
    "            # Add new words to sequences\n",
    "            seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n",
    "\n",
    "            # Which sequences are incomplete (didn't reach <end>)?\n",
    "            incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n",
    "                               next_word != word_map['<end>']]\n",
    "            complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
    "\n",
    "            # Set aside complete sequences\n",
    "            if len(complete_inds) > 0:\n",
    "                complete_seqs.extend(seqs[complete_inds].tolist())\n",
    "                complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
    "            k -= len(complete_inds)  # reduce beam length accordingly\n",
    "\n",
    "            # Proceed with incomplete sequences\n",
    "            if k == 0:\n",
    "                break\n",
    "            seqs = seqs[incomplete_inds]\n",
    "            h1 = h1[prev_word_inds[incomplete_inds]]\n",
    "            c1 = c1[prev_word_inds[incomplete_inds]]\n",
    "            h2 = h2[prev_word_inds[incomplete_inds]]\n",
    "            c2 = c2[prev_word_inds[incomplete_inds]]\n",
    "            image_features_mean = image_features_mean[prev_word_inds[incomplete_inds]]\n",
    "            top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
    "            k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
    "\n",
    "            # Break if things have been going on too long\n",
    "            if step > 50:\n",
    "                break\n",
    "            step += 1\n",
    "\n",
    "        i = complete_seqs_scores.index(max(complete_seqs_scores))\n",
    "        seq = complete_seqs[i]\n",
    "\n",
    "        # References\n",
    "        img_caps = allcaps[0].tolist()\n",
    "        img_captions = list(\n",
    "            map(lambda c: [rev_word_map[w] for w in c if w not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}],\n",
    "                img_caps))  # remove <start> and pads\n",
    "        img_caps = [' '.join(c) for c in img_captions]\n",
    "        #print(img_caps)\n",
    "        references.append(img_caps)\n",
    "\n",
    "        # Hypotheses\n",
    "        hypothesis = ([rev_word_map[w] for w in seq if w not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}])\n",
    "        hypothesis = ' '.join(hypothesis)\n",
    "        #print(hypothesis)\n",
    "        hypotheses.append(hypothesis)\n",
    "        assert len(references) == len(hypotheses)\n",
    "    return complete_seqs_scores, complete_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EVALUATING AT BEAM SIZE 5:   0%|          | 1/25000 [00:00<1:05:41,  6.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_features.shape: torch.Size([1, 36, 2048])\n",
      "np.min(image_features): tensor(0.), np.max(image_features): tensor(32.9999)\n",
      "image_features_mean.shape: torch.Size([1, 2048])\n",
      "image_features_mean.shape: torch.Size([5, 2048])\n",
      "image_features.shape: torch.Size([1, 36, 2048])\n",
      "np.min(image_features): tensor(0.), np.max(image_features): tensor(27.5576)\n",
      "image_features_mean.shape: torch.Size([1, 2048])\n",
      "image_features_mean.shape: torch.Size([5, 2048])\n",
      "image_features.shape: torch.Size([1, 36, 2048])\n",
      "np.min(image_features): tensor(0.), np.max(image_features): tensor(26.7169)\n",
      "image_features_mean.shape: torch.Size([1, 2048])\n",
      "image_features_mean.shape: torch.Size([5, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EVALUATING AT BEAM SIZE 5:   0%|          | 3/25000 [00:00<42:50,  9.72it/s]  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([tensor(-6.9773, device='cuda:0', grad_fn=<SelectBackward>),\n",
       "  tensor(-6.4317, device='cuda:0', grad_fn=<SelectBackward>),\n",
       "  tensor(-7.2896, device='cuda:0', grad_fn=<SelectBackward>),\n",
       "  tensor(-7.6408, device='cuda:0', grad_fn=<SelectBackward>),\n",
       "  tensor(-7.6486, device='cuda:0', grad_fn=<SelectBackward>)],\n",
       " [[9488, 7961, 5081, 1419, 2599, 7961, 9157, 8038, 5310, 9489],\n",
       "  [9488, 7961, 5081, 1419, 2599, 7961, 9157, 290, 7961, 5310, 9489],\n",
       "  [9488,\n",
       "   7961,\n",
       "   5081,\n",
       "   1419,\n",
       "   2599,\n",
       "   7961,\n",
       "   9157,\n",
       "   290,\n",
       "   7961,\n",
       "   9158,\n",
       "   4437,\n",
       "   5310,\n",
       "   9489],\n",
       "  [9488, 7961, 5081, 1419, 2599, 7961, 9157, 290, 7961, 5310, 702, 3632, 9489],\n",
       "  [9488, 7961, 5081, 1419, 2599, 7961, 9157, 290, 7961, 72, 4437, 5310, 9489]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vqa",
   "language": "python",
   "name": "vqa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
